# NETS213 HW6: Replicate Science
Contributors: Molly Wang, Emma Hong, Gaston Montemayor, Nick Newberg

##1. Literature Review

#####Demographics of Mechanical Turk
This study was conducted over 3 weeks in order to collect demographic information about Mechanical Turk workers. Each participant was paid $0.10 to fill out a survey of 10 questions regarding gender, age, education, marital status, children, household size, engagement and motivation. The results showed that a majority of workers were female, young, well-educated, earned lower incomes, single without children and participated for the monetary reward rather than killing time. 

#####Labelling Images With Computer Games
Researchers from Carnegie Mellon University created a 'fun' computer game that functioned as a crowdsourcing tool to label images. The game is played with a randomly assigned partner and the goal is to guess what your partner is typing about a given image displayed. Once both players type the same word, they move on to the next image. They have 2.5 minutes to agree on as many images as possible and are rewarded with points on each success. This incentivizes them to keep playing. Taboo words are generated by the first guesses of the first instance of an image in the game. After they are generated, future partners who have to analyze the image may not use those words in the description. This guarantees that images will have many labels associated with it. A label threshold is implemented which defines the number of times pairs agree upon a word before it becomes a label for a particular image. Once an image has acquired a large amount of taboo words it is fully labeled and no longer in the game. These fully labeled may re-enter the game several months later to be re-labelled because language changes over time. 350,000 images are selected at random from Google, inserted into the game, and once they are labelled the next set of 350,000 words are collected. Players can also choose to enter theme rooms where the images are of a unique category. This allows for more specific labelling. 13,630 people played the game and 1.3 million labels were collected.

#####Exploring Iterative and Parallel Human Computation Processes
For this assignment, we chose to replicate *“Exploring Iterative and Parallel Human Computation Processes”* by Greg Little, Lydia B. Chilton, Max Goldman, and Robert C. Miller. In the paper, the researchers use crowdsourcing to compare iterative and parallel human computation processes in various situations. Iterative processing is when workers continue to work off each other’s work. On the other hand, in parallel processing, workers work alone on tasks. The problem domains examined in the paper are writing, transcription, and brainstorming. Results of the experiment indicated that iterative processing boosts the average quality of the work from writing and brainstorming tasks; however, in brainstorming and transcription, parallel processing may be the better choice because a wide variety of responses is desired. 

##2. In-Depth Explanation of the Paper’s Experimental Design
The researchers set out three different experiments, each of which will be discussed below, to measure how well contributors could explain things literally, creatively, and when faced when challenges. In order to determine whether iterative or parallel tasks (defined above) generate a higher quality of work, this experiment separated work into:

1. *Creation Tasks*: users create their own content 
2. *Decision Tasks*: users rate and judge others’ content, using these solicited opinions to then determine which content is the “best”
3. *Combined Tasks*: users build upon each other’s content and rate each other 

#####Experiment 1: Writing Image Descriptions
The first experiment asked contributors to label images factually (i.e. what is literally going on in the picture). Contributors had to view and describe an image for $0.02, and these contributors were recruited using Amazon Mechanical Turk. There were six different images that people could label. This was the creation task. The decision task asked a different set of contributors to vote for the five best descriptions for $0.01 on scale from 1-10. Contributors could not be a part of both the creation and decision tasks. This experiment was done both in parallel and iteratively. The iterative phase involved contributors improving upon past descriptions written by other users. Researchers discovered that iteratively generated descriptions were rated higher on average than those generated in parallel. They also noted that there may be a correlation with description length and rating as well; longer descriptions were usually the product of iterative tasks. These results were shown using a bar graph comparing average rating of the iterative and parallel processing conditions across iterations. The researchers also plotted the length of description to the rating using a linear regression, which has a positive correlation. 

#####Experiment 2: Brainstorming
The second experiment measured contributors’ creativity by asking them to come up with potential company names after reading a short description of the company’s services. Like the last experiment, it paid users $0.02 to “create” five new company name ideas and then paid a separate set of users to rate these company names. In the parallel task, contributors only saw a description of the company and were asked to generate five names. In the iterative task, contributors saw names that other users had already suggested. The parallel task generated on average higher rated names as they tended to be more creative and less like names already generated by others. However, it was interesting to note that the average iteratively generated company names were rated higher (names that contributors marked in between the best and worst). The researchers concluded that individual brainstorming sessions (parallel) generated more creative results but that group ones (iterative) tended to generate names that were overall more of an average fit. 

The results were visualized using a bar graph that shows average ratings given to names across iterations of the iterative processing condition, and the bar graph also shows standard error of the names generated. Spanning across the bars of the bar graph is the average rating and standard error of names from the parallel brainstorming condition. The researchers also modeled the probability of generating names with various ratings in both the iterative and parallel processing conditions with Gaussian distributions. This distribution visualization demonstrates that the iterative process led to a higher average in ratings, while the parallel process has a wider variance in ratings. 

#####Experiment 3: Blurry Text Recognition
For the blurry text recognition experiment, 16 creation tasks were used in both the iterative and parallel processes. Users were paid $.05 upon completion of a task. These tasks involved being given blurry text and transcribing what the text says. The blurry text is obtained through writing 12 original passages and then blurring the text using image filters. The researchers thought it was important to have original passages because otherwise workers could search online for the passage. Beneath each word, there is a textbox. In the iterative processing condition, the most recent guesses for that word is displayed. Workers indicate words they are unsure about with a ‘*’ in both the iterative and parallel conditions. No turkers were allowed to work on the tasks in both the iterative and the parallel conditions. In the sixteen iterations of the task, words that were guessed most were chosen in the analysis of how successful the different strategies were. 

Averaging over the 12 passages, the researchers did not find a statistically significant difference in performance between the iterative and parallel processing conditions. However, turkers spent less time on the task when in the iterative processing condition than in the parallel processing condition. Using the extraction algorithm from the experiment, the iterative processing condition seems marginally better. One danger found in the results is that in the iterative processing condition, past work may lead future workers away from the correct result. These results were visualized using bar graphs comparing the accuracy among the iterative and parallel conditions across iterations, along with error bars to show standard error. 

##3. Why We Chose the Brainstorming Experiment in *Exploring Iterative and Parallel Human Computation Processes*
The specific experiment we chose to replicate was the brainstorming task. We were interested in replicating the research findings because the paper indicated that brainstorming with iterative processing had higher average quality, while parallel processing had the best results. This conflict of whether iterative or parallel processing is better for brainstorming was an area we wanted to examine in greater depth. Moreover, we thought using brainstorming as our experiment would be interesting in understanding how researchers can quantify creativity. 

##4. Experimental Setup

For our experiment, we decided to duplicate the second experiment, which involves brainstorming company names. However, we took some creative liberty in the replication of this experiment and instead of brainstorming company names, we changed the task to ask users to brainstorm potential band names based on a description of the band members, the type of music they produce, and their hometown.

We also separated our tasks into *creation tasks* and *decision tasks.* 

**Example Creation Task - Iteration 1**

**Type of music we make**: Hardcore techno 

**We are from**: the future 

**Instruments we play**: synthesizer and our voices 

**Who we are**: We are four robots from the future that felt that we needed to bring some musical emotion to 2016’s boring repetitive music. We are strong believers that robots can make better music than humans! 


**Example Creation Task - Iteration 2**

**Type of music**: Pop music

**We are from**: Seoul, South Korea

**Instruments we play**: Autotune

**Who we are**: We are five 20 year old boys with the biggest number of teenage fans in South East Asia. We’ve signed over 1,000,000 autographs for pretty girls. We love dancing and are so grateful for our fans!


Choosing to have contributors brainstorm band names was a slight deviation from the original experiment, but the nature of the task is very similar. Furthermore, we thought it would be interesting to see the variety of band names that we could get. 

![Name the Music Group](https://github.com/mollywang/NETS213-Crowdsourcing-HW6/blob/master/namethemusicgroup.png)
This is an example of what an *iterative creation task* looks like to a contributor. They would see 10 different text boxes in which they had to input their results.

The creation task was the creation of the band names, and the decision task was the ratings of the band names that were collected in the iterative and parallel processes. Our iterative processing setup deviated from the original experiment for the sake of simplicity -- instead of having a list of names suggested in all past iterations, we selected ten random names created in the parallel process. For the decision task, participants rated the previously generated names on a scale from 1 to 10. 

##5. Platform and Task Design Details
We used Crowdflower and only allowed Level 3 workers to participate in our creation tasks and only allowed Level 2 workers to participate in our decision tasks. Each contributor was paid $0.05 to generate 5 unique names for a music group. After these were collected, voters were paid $0.01 to rank 10 the previously generated names. We also incentivized participants to put effort in the name generation by offering a bonus of $2 if one of their names was ranked the highest. The name generation task warned that if workers used existing band names, they would not receive payment. The voting task included information on how to determine “how good” a given name was. Scores of 1 indicated that the name was incoherent, unrelated, or unoriginal. Scores of 10 indicated exceptional names that were original, creative and related to the description of the band. Below this disclaimer we included a description of the group and then had 5 voting fields that linked to unique names. Both name generators and voters could not leave any field blank. Any given generated name was voted on 10 times and each participant was allowed to vote a maximum of 6 times (60 individual votes) and generate names 3 times (15 individual names). 

![Rate the Music Group](https://github.com/mollywang/NETS213-Crowdsourcing-HW6/blob/master/ratethemusicgroup.png)
![Rate the Music Group 2](https://github.com/mollywang/NETS213-Crowdsourcing-HW6/blob/master/ratemusicgroup2.png)
This is an example of what a *decision task* looks like to a contributor.


##6. Analysis of Results and Comparison to the Findings of the Original Paper
###6.1 *Results and Discussion*

![Iterative Results](https://github.com/mollywang/NETS213-Crowdsourcing-HW6/blob/master/iterative-new.png)
![Parallel Results](https://github.com/mollywang/NETS213-Crowdsourcing-HW6/blob/master/parallel-new.png)

####6.1.1 *Getting the Best Names*
Finding the ‘best’ band name is not a trivial or objective task. Generating unique names requires some level of creativity, and it is extremely difficult to come to a unanimous decision on something that is the most creative. Instead of brainstorming company names, we had participants generate music group names. While company names are semi-objective, proper and socially acceptable; band names are not. Although some crowd workers ignored our instructions and used existing names, googled names or copy and pasted from the descriptions, many were creative and took liberty in the process. More so than the company names, there were some cool, funny and abstract band names that required unique thinking to generate. The results allowed us to see real people behind the data. Allowing this amount of freedom also comes with shortcomings. Some contributors used inappropriate or offensive words such as *“Rape Yelp”* or *“Dick of All”*, which also was an issue in the company name experiment. We also noted that there were clever names that received low votes. For the robot band, someone suggested the name *“1101”*, which many most likely saw as gibberish, but was indeed clever because it looks like binary code. These types of abstract connections make sentiment analysis on subjective data like this much more difficult than on something like company names. Voters also seemed to vote higher on more-positive names like *“Heaven’s Angels”* and lower on negative names like *“Dick of All”*. It seems as if they were searching for objective quantifiers to classify something subjective. Even if we can’t rely fully on crowdworkers to rank the results, crowdsourcing creative problem-solving may still be compelling. It might make more sense for the poster of the task to pick the best in the end because their idea of ‘best’ ultimately matters the most.

###6.2 *Potential Errors*
Rather than using real-time iteration, we inserted the results from our parallel tasks in the iterative tasks. This may have skewed our data because we did not allow for the organic flow that a true iterative task incorporates. We also did not take any measures in preventing contributors from using the same name multiple times. One worker wrote “K-Pop” for every single description. As with the original experiment, our contributors are crowdworkers and therefore not that diverse.




