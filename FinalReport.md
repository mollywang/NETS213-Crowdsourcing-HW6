# NETS213 HW6: Replicate Science
Contributors: Molly Wang, Emma Hong, Gaston Montemayor, Nick Newberg

##1. Literature Review

#####Demographics of Mechanical Turk
This study was conducted over 3 weeks in order to collect demographic information about Mechanical Turk workers. Each participant was paid $0.10 to fill out a survey of 10 questions regarding gender, age, education, marital status, children, household size, engagement and motivation. The results showed that a majority of workers were female, young, well-educated, earned lower incomes, single without children and participated for the monetary reward rather than killing time. 

#####Labelling Images With Computer Games
Researchers from Carnegie Mellon University created a 'fun' computer game that functioned as a crowdsourcing tool to label images. The game is played with a randomly assigned partner and the goal is to guess what your partner is typing about a given image displayed. Once both players type the same word, they move on to the next image. They have 2.5 minutes to agree on as many images as possible and are rewarded with points on each success. This incentivizes them to keep playing. Taboo words are generated by the first guesses of the first instance of an image in the game. After they are generated, future partners who have to analyze the image may not use those words in the description. This guarantees that images will have many labels associated with it. A label threshold is implemented which defines the number of times pairs agree upon a word before it becomes a label for a particular image. Once an image has acquired a large amount of taboo words it is fully labeled and no longer in the game. These fully labeled may re-enter the game several months later to be re-labelled because language changes over time. 350,000 images are selected at random from Google, inserted into the game, and once they are labelled the next set of 350,000 words are collected. Players can also choose to enter theme rooms where the images are of a unique category. This allows for more specific labelling. 13,630 people played the game and 1.3 million labels were collected.

#####Exploring Iterative and Parallel Human Computation Processes
For this assignment, we chose to replicate *“Exploring Iterative and Parallel Human Computation Processes”* by Greg Little, Lydia B. Chilton, Max Goldman, and Robert C. Miller. In the paper, the researchers use crowdsourcing to compare iterative and parallel human computation processes in various situations. Iterative processing is when workers continue to work off each other’s work. On the other hand, in parallel processing, workers work alone on tasks. The problem domains examined in the paper are writing, transcription, and brainstorming. Results of the experiment indicated that iterative processing boosts the average quality of the work from writing and brainstorming tasks; however, in brainstorming and transcription, parallel processing may be the better choice because a wide variety of responses is desired. 

##2. In-Depth Explanation of the Paper’s Experimental Design
The researchers set out three different experiments, each of which will be discussed below, to measure how well contributors could explain things literally, creatively, and when faced when challenges. In order to determine whether iterative or parallel tasks (defined above) generate a higher quality of work, this experiment separated work into:

1. *Creation Tasks*: users create their own content 
2. *Decision Tasks*: users rate and judge others’ content, using these solicited opinions to then determine which content is the “best”
3. *Combined Tasks*: users build upon each other’s content and rate each other 

#####Experiment 1: Writing Image Descriptions
The first experiment asked contributors to label images factually (i.e. what is literally going on in the picture). Contributors had to view and describe an image for $0.02. There were six different images that people could label. This was the creation task. The decision task asked a different set of contributors to vote for the five best descriptions for $0.01 on scale from 1-10. Contributors could not be a part of both the creation and decision tasks. This experiment was done both in parallel and iteratively. The iterative phase involved contributors improving upon past descriptions written by other users. Researchers discovered that iteratively generated descriptions were rated higher on average than those generated in parallel. They also noted that there may be a correlation with description length and rating as well; longer descriptions were usually the product of iterative tasks.

#####Experiment 2: Brainstorming
The second experiment measured contributors’ creativity by asking them to come up with potential company names after reading a short description of the company’s services. Like the last experiment, it paid users $0.02 to “create” five new company name ideas and then paid a separate set of users to rate these company names. In the parallel task, contributors only saw a description of the company and were asked to generate five names. In the iterative task, contributors saw names that other users had already suggested. The parallel task generated on average higher rated names as they tended to be more creative and less like names already generated by others. However, it was interesting to note that the average iteratively generated company names were rated higher (names that contributors marked in between the best and worst). The researchers concluded that individual brainstorming sessions (parallel) generated more creative results but that group ones (iterative) tended to generate names that were overall more of an average fit.

#####Experiment 3: Blurry Text Recognition
For the blurry text recognition experiment, 16 creation tasks were used in both the iterative and parallel processes. Completion of the task These tasks involved being given blurry text and transcribing what the text says. The blurry text is obtained through writing 12 original passages and then blurring the text using image filters. Beneath each word, there is a textbox. In the iterative processing condition, the most recent guesses for that word is displayed. Workers indicate words they are unsure about with a ‘*’ in both the iterative and parallel conditions. No turkers were allowed to work on the tasks in both the iterative and the parallel conditions. In the sixteen iterations of the task, words that were guessed most were chosen in the analysis of how successful the different strategies were. 
Averaging over the 12 passages, the researchers did not find a statistically significant difference in performance between the iterative and parallel processing conditions. However, turkers spent less time on the task when in the iterative processing condition than in the parallel processing condition. Using the extraction algorithm from the experiment, the iterative processing condition seems marginally better. One danger found in the results is that in the iterative processing condition, past work may lead future workers away from the correct result. 

##3. Why We Chose the Brainstorming Experiment in *Exploring Iterative and Parallel Human Computation Processes*
The specific experiment we chose to replicate was the brainstorming task. We were interested in replicating the research findings because the paper indicated that brainstorming with iterative processing had higher average quality, while parallel processing had the best results. This conflict of whether iterative or parallel processing is better for brainstorming was an area we wanted to examine in greater depth. Moreover, we thought using brainstorming as our experiment would be interesting in understanding how researchers can quantify creativity. 
A description of your experimental setup, along with an explanation of any deviations that you made from the original design
For our experiment, we decided to duplicate the second experiment, which involves brainstorming company names. However, we took some creative liberty in the replication of this experiment and instead of brainstorming company names, we changed the task to ask users to brainstorm potential band names based on a description of the band members, the type of music they produce, and their hometown.

Example (copy paste screenshot of task on CrowdFlower here) 
Type of music we make: Hardcore techno
We are from: the future
Instruments we play: synthesizer and our voices
Who we are: We are four robots from the future that felt that we needed to bring some musical emotion to 2016’s boring repetitive music. We are strong believers that robots can make better music than humans!
Choosing to have contributors brainstorm band names was a slight deviation from the original experiment, but the nature of the task is very similar. Furthermore, we thought it would be interesting to see the variety of band names that we could get. 
A description of what crowdsourcing platform you used, a description of the selection criteria you used for workers, how much you paid, and how you decided to reject workers (if applicable). You should also give a pointer to your task design, which can be in a separate document. (Everyone) 

##4. Analysis of Results and Comparison to the Findings of the Original Paper

